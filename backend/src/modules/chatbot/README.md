# M√≥dulo Chatbot con Llama 3.2 - Alcald√≠a de Montebello

Este m√≥dulo implementa un sistema de chatbot inteligente para la Alcald√≠a de Montebello, potenciado por **Llama 3.2**, dise√±ado para asistir a campesinos, funcionarios y administradores con informaci√≥n sobre programas municipales, tr√°mites y servicios.

## üöÄ Caracter√≠sticas Principales

### ü§ñ Asistente Virtual con IA
- **Llama 3.2** como motor principal de respuestas
- Respuestas contextuales y naturales
- Validaci√≥n de contexto espec√≠fico para Montebello
- Fallback a respuestas predefinidas
- Base de conocimiento espec√≠fica para servicios municipales

### üí¨ Gesti√≥n de Sesiones
- Sesiones persistentes con historial de conversaci√≥n
- M√∫ltiples sesiones activas por usuario
- Cierre autom√°tico de sesiones inactivas
- Archivado autom√°tico de conversaciones antiguas

### üë• Soporte Multi-Usuario
- **Campesinos**: Informaci√≥n sobre programas agr√≠colas, insumos, capacitaciones
- **Funcionarios**: Gesti√≥n de solicitudes, reportes, procedimientos internos
- **Administradores**: M√©tricas del sistema, gesti√≥n de usuarios, configuraci√≥n

### üîí Seguridad y Validaci√≥n
- Validaci√≥n de consultas relacionadas con Montebello
- Filtrado de contenido no relacionado
- Rate limiting para prevenir spam
- Autenticaci√≥n requerida para todas las operaciones

## üß† Integraci√≥n con Llama 3.2

### Configuraci√≥n del Modelo

```javascript
// Configuraci√≥n en chatbot.config.js
LLAMA: {
    MODEL_NAME: 'llama3.2',
    API_URL: process.env.LLAMA_API_URL || 'http://localhost:11434/api/generate',
    MAX_TOKENS: 512,
    TEMPERATURE: 0.7,
    TOP_P: 0.9,
    TIMEOUT_MS: 30000,
    RETRY_ATTEMPTS: 3,
    CONTEXT_WINDOW: 4096
}
```

### Variables de Entorno

```env
# Configuraci√≥n de Llama 3.2
LLAMA_API_URL=http://localhost:11434/api/generate

# Base de datos
MONGO_URI=mongodb://localhost:27017/alcaldia_montebello

# Configuraci√≥n del servidor
PORT=5000
NODE_ENV=development

# JWT para autenticaci√≥n
JWT_SECRET=tu_jwt_secret_aqui
JWT_EXPIRE=30d
```

### Contexto Espec√≠fico de Montebello

El sistema incluye un prompt especializado que instruye a Llama 3.2 sobre:

- **Municipio**: Montebello, Antioquia, Colombia
- **Actividades principales**: Agricultura (caf√©, hortalizas, flores), ganader√≠a menor, turismo rural
- **Programas disponibles**: 5 programas municipales espec√≠ficos
- **Restricciones**: Solo responde sobre temas relacionados con Montebello
- **Tono**: Profesional pero cercano y amigable

## üìÅ Estructura del M√≥dulo

```
chatbot/
‚îú‚îÄ‚îÄ chatbot.config.js      # Configuraci√≥n del m√≥dulo y Llama 3.2
‚îú‚îÄ‚îÄ chatbot.controller.js  # Controladores de endpoints
‚îú‚îÄ‚îÄ chatbot.middleware.js  # Middlewares de validaci√≥n y seguridad
‚îú‚îÄ‚îÄ chatbot.model.js       # Modelo de datos de sesiones
‚îú‚îÄ‚îÄ chatbot.routes.js      # Definici√≥n de rutas
‚îú‚îÄ‚îÄ chatbot.service.js     # L√≥gica de negocio principal
‚îú‚îÄ‚îÄ chatbot.utils.js       # Utilidades y helpers
‚îú‚îÄ‚îÄ chatbot.validation.js  # Validaciones de entrada
‚îú‚îÄ‚îÄ llama.service.js       # Servicio espec√≠fico para Llama 3.2
‚îî‚îÄ‚îÄ README.md             # Documentaci√≥n
```

## üîó API Endpoints

### Gesti√≥n de Sesiones

#### `POST /api/chatbot/session/start`
Inicia una nueva sesi√≥n de chat o retorna una sesi√≥n activa existente.

#### `PUT /api/chatbot/session/:sessionId/close`
Cierra una sesi√≥n espec√≠fica.

#### `GET /api/chatbot/session/:sessionId`
Obtiene los detalles de una sesi√≥n espec√≠fica.

### Mensajer√≠a con IA

#### `POST /api/chatbot/message`
Env√≠a un mensaje al chatbot y recibe una respuesta generada por Llama 3.2.

**Cuerpo de la petici√≥n:**
```json
{
  "message": "¬øC√≥mo puedo solicitar semillas de caf√© en Montebello?",
  "sessionId": "chat_1234567890_abc123"
}
```

**Respuesta:**
```json
{
  "success": true,
  "message": "Mensaje procesado correctamente",
  "data": {
    "response": "Para solicitar semillas de caf√© en Montebello, puedes seguir estos pasos: 1. Ingresa al portal de la alcald√≠a...",
    "sessionId": "chat_1234567890_abc123",
    "timestamp": "2024-01-15T10:30:00.000Z"
  }
}
```

### Nuevos Endpoints para Llama 3.2

#### `GET /api/chatbot/llama/status`
Verifica el estado del servicio de Llama 3.2.

**Respuesta:**
```json
{
  "success": true,
  "data": {
    "available": true,
    "model": "llama3.2",
    "api_url": "http://localhost:11434/api/generate",
    "ai_responses_enabled": true,
    "fallback_enabled": true
  }
}
```

#### `POST /api/chatbot/admin/toggle-ai` (Solo Administradores)
Alterna entre respuestas de IA y respuestas predefinidas.

**Cuerpo de la petici√≥n:**
```json
{
  "enabled": true
}
```

#### `GET /api/chatbot/config`
Obtiene la configuraci√≥n actual del chatbot.

### Utilidades

#### `GET /api/chatbot/quick-replies`
Obtiene respuestas r√°pidas personalizadas seg√∫n el tipo de usuario.

#### `GET /api/chatbot/sessions`
Obtiene el historial de sesiones del usuario.

#### `GET /api/chatbot/stats`
Obtiene estad√≠sticas de uso del chatbot para el usuario.

## ‚öôÔ∏è Configuraci√≥n de Llama 3.2

### Instalaci√≥n de Ollama (Recomendado)

1. **Instalar Ollama:**
```bash
# En Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# En Windows
# Descargar desde https://ollama.ai/download
```

2. **Descargar Llama 3.2:**
```bash
ollama pull llama3.2
```

3. **Iniciar el servicio:**
```bash
ollama serve
```

4. **Verificar instalaci√≥n:**
```bash
curl http://localhost:11434/api/tags
```

### Configuraci√≥n Alternativa

Si usas otro proveedor de Llama 3.2, actualiza la URL en las variables de entorno:

```env
LLAMA_API_URL=https://tu-proveedor-llama.com/api/generate
```

## üéØ Validaci√≥n de Contexto

El sistema valida que las consultas est√©n relacionadas con Montebello mediante:

### Palabras Clave Municipales
- T√©rminos relacionados con servicios municipales
- Programas espec√≠ficos de la alcald√≠a
- Actividades agr√≠colas locales
- Informaci√≥n de contacto y tr√°mites

### Respuestas de Redirecci√≥n
Si una consulta no est√° relacionada con Montebello, el sistema redirige amablemente:

```
"üèõÔ∏è Soy el asistente virtual de la Alcald√≠a de Montebello. 
Estoy aqu√≠ para ayudarte con informaci√≥n sobre nuestros 
programas municipales, tr√°mites y servicios. 
¬øEn qu√© puedo asistirte hoy?"
```

## üîÑ Sistema de Fallback

El chatbot implementa un sistema robusto de fallback:

1. **Llama 3.2** (Principal): Respuestas generadas por IA
2. **Respuestas Predefinidas** (Fallback): Base de conocimiento local
3. **Mensaje de Error** (√öltimo recurso): Cuando ambos fallan

### Configuraci√≥n del Fallback

```javascript
// En chatbot.config.js
RESPONSES: {
    USE_AI_RESPONSES: true,        // Usar Llama 3.2
    FALLBACK_TO_PREDEFINED: true   // Fallback activado
}
```

## üìä Monitoreo y M√©tricas

### M√©tricas de IA
- Tiempo de respuesta de Llama 3.2
- Tokens utilizados por consulta
- Tasa de √©xito/fallo de la IA
- Uso de fallback vs respuestas de IA

### Logs Espec√≠ficos
```javascript
// Ejemplo de log de respuesta
console.log(`Respuesta generada por llama: Para solicitar semillas...`);
console.warn('Llama service failed, falling back to predefined responses: Connection timeout');
```

## üõ†Ô∏è Desarrollo y Extensi√≥n

### Modificar el Prompt del Sistema

Edita `SYSTEM_PROMPT` en `chatbot.config.js` para ajustar el comportamiento de Llama 3.2:

```javascript
export const SYSTEM_PROMPT = `Eres un asistente virtual de la Alcald√≠a de Montebello...
// Agregar nuevas instrucciones aqu√≠
`;
```

### Agregar Nuevas Validaciones

Modifica `isValidQuery()` en `llama.service.js` para incluir nuevas palabras clave:

```javascript
const municipalKeywords = [
    // Palabras existentes...
    'nueva_palabra_clave',
    'otro_termino_municipal'
];
```

### Configurar Par√°metros de IA

Ajusta los par√°metros de Llama 3.2 en `chatbot.config.js`:

```javascript
LLAMA: {
    TEMPERATURE: 0.7,    // Creatividad (0.0 - 1.0)
    TOP_P: 0.9,         // Diversidad de respuestas
    MAX_TOKENS: 512     // Longitud m√°xima de respuesta
}
```

## üîß Soluci√≥n de Problemas

### Problemas con Llama 3.2

1. **Servicio no disponible**
   ```bash
   # Verificar que Ollama est√© ejecut√°ndose
   curl http://localhost:11434/api/tags
   
   # Reiniciar Ollama si es necesario
   ollama serve
   ```

2. **Modelo no encontrado**
   ```bash
   # Descargar el modelo
   ollama pull llama3.2
   ```

3. **Timeouts frecuentes**
   - Aumentar `TIMEOUT_MS` en la configuraci√≥n
   - Verificar recursos del servidor
   - Considerar usar un modelo m√°s peque√±o

4. **Respuestas fuera de contexto**
   - Revisar y ajustar `SYSTEM_PROMPT`
   - Mejorar validaciones en `isValidQuery()`
   - Agregar m√°s palabras clave espec√≠ficas

### Logs de Debug

```javascript
// Habilitar logs detallados
LOGGING_CONFIG.LOG_LEVEL = 'debug';
```

### Verificar Estado del Sistema

```bash
# Endpoint para verificar estado
GET /api/chatbot/llama/status

# Endpoint para configuraci√≥n
GET /api/chatbot/config
```

## üöÄ Despliegue en Producci√≥n

### Consideraciones de Rendimiento

1. **Recursos del Servidor**
   - CPU: M√≠nimo 4 cores para Llama 3.2
   - RAM: M√≠nimo 8GB (16GB recomendado)
   - Almacenamiento: SSD recomendado

2. **Configuraci√≥n de Producci√≥n**
   ```env
   NODE_ENV=production
   LLAMA_API_URL=http://llama-server:11434/api/generate
   ```

3. **Monitoreo**
   - Configurar alertas para fallos de IA
   - Monitorear uso de recursos
   - Logs de rendimiento activados

### Docker Compose (Ejemplo)

```yaml
version: '3.8'
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    
  chatbot:
    build: .
    environment:
      - LLAMA_API_URL=http://ollama:11434/api/generate
    depends_on:
      - ollama
```

## üìù Licencia

Este m√≥dulo es parte del sistema de la Alcald√≠a de Montebello y est√° sujeto a las pol√≠ticas de software de la instituci√≥n.